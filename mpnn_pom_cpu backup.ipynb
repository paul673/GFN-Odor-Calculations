{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to run this file\n",
        "\n",
        "Create a virtual environment:\n",
        "\n",
        "`py -3.9 -m venv venv`\n",
        "\n",
        "\n",
        "Activate the environment \n",
        "\n",
        "`.\\venv\\Scripts\\activate`\n",
        "\n",
        "Install working version of dgl\n",
        "\n",
        "`pip install dgl-2.0.0-cp39-cp39-win_amd64.whl`\n",
        "\n",
        "Install openpom\n",
        "\n",
        "`pip install openpom`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gONoc9hH_n13",
        "outputId": "7acb96d7-4dba-4f1b-ded6-c1b3591467b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No normalization for SPS. Feature removed!\n",
            "No normalization for AvgIpc. Feature removed!\n",
            "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
            "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
            "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (d:\\Dokumenter\\Skole\\Prosjektoppgave\\prosjekt\\venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
            "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (d:\\Dokumenter\\Skole\\Prosjektoppgave\\prosjekt\\venv\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
            "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
            "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
            "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
          ]
        }
      ],
      "source": [
        "#!pip install openpom\n",
        "#!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/repo.html\n",
        "import deepchem as dc\n",
        "from openpom.feat.graph_featurizer import GraphFeaturizer, GraphConvConstants\n",
        "from openpom.utils.data_utils import get_class_imbalance_ratio\n",
        "#from openpom.models.mpnn_pom import MPNNPOMModel\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
        "\n",
        "from deepchem.models.losses import Loss, L2Loss\n",
        "from deepchem.models.torch_models.torch_model import TorchModel\n",
        "from deepchem.models.optimizers import Optimizer, LearningRateSchedule\n",
        "\n",
        "from openpom.layers.pom_ffn import CustomPositionwiseFeedForward\n",
        "from openpom.utils.loss import CustomMultiLabelLoss\n",
        "from openpom.utils.optimizer import get_optimizer\n",
        "\n",
        "try:\n",
        "    import dgl\n",
        "    from dgl import DGLGraph\n",
        "    from dgl.nn.pytorch import Set2Set\n",
        "    from openpom.layers.pom_mpnn_gnn import CustomMPNNGNN\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    raise ImportError('This module requires dgl and dgllife')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05soBOnzKQBh",
        "outputId": "90a4a46e-74d2-432e-a45c-bca887eff6b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class MPNNPOM(nn.Module):\n",
        "    \"\"\"\n",
        "    MPNN model computes a principal odor map\n",
        "    using multilabel-classification based on the pre-print:\n",
        "    \"A Principal Odor Map Unifies DiverseTasks in Human\n",
        "        Olfactory Perception\" [1]\n",
        "\n",
        "    This model proceeds as follows:\n",
        "\n",
        "    * Combine latest node representations and edge features in\n",
        "        updating node representations, which involves multiple\n",
        "        rounds of message passing.\n",
        "    * For each graph, compute its representation by radius 0 combination\n",
        "        to fold atom and bond embeddings together, followed by\n",
        "        'set2set' or 'global_sum_pooling' readout.\n",
        "    * Perform the final prediction using a feed-forward layer.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Brian K. Lee, Emily J. Mayhew, Benjamin Sanchez-Lengeling,\n",
        "        Jennifer N. Wei, Wesley W. Qian, Kelsie Little, Matthew Andres,\n",
        "        Britney B. Nguyen, Theresa Moloy, Jane K. Parker, Richard C. Gerkin,\n",
        "        Joel D. Mainland, Alexander B. Wiltschko\n",
        "        `A Principal Odor Map Unifies Diverse Tasks\n",
        "        in Human Olfactory Perception preprint\n",
        "        <https://www.biorxiv.org/content/10.1101/2022.09.01.504602v4>`_.\n",
        "\n",
        "    .. [2] Benjamin Sanchez-Lengeling, Jennifer N. Wei, Brian K. Lee,\n",
        "        Richard C. Gerkin, Alán Aspuru-Guzik, Alexander B. Wiltschko\n",
        "        `Machine Learning for Scent:\n",
        "        Learning Generalizable Perceptual Representations\n",
        "        of Small Molecules <https://arxiv.org/abs/1910.10685>`_.\n",
        "\n",
        "    .. [3] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,\n",
        "        Oriol Vinyals, George E. Dahl.\n",
        "        \"Neural Message Passing for Quantum Chemistry.\" ICML 2017.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This class requires DGL (https://github.com/dmlc/dgl)\n",
        "    and DGL-LifeSci (https://github.com/awslabs/dgl-lifesci)\n",
        "    to be installed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_tasks: int,\n",
        "                 node_out_feats: int = 64,\n",
        "                 edge_hidden_feats: int = 128,\n",
        "                 edge_out_feats: int = 64,\n",
        "                 num_step_message_passing: int = 3,\n",
        "                 mpnn_residual: bool = True,\n",
        "                 message_aggregator_type: str = 'sum',\n",
        "                 mode: str = 'classification',\n",
        "                 number_atom_features: int = 134,\n",
        "                 number_bond_features: int = 6,\n",
        "                 n_classes: int = 1,\n",
        "                 nfeat_name: str = 'x',\n",
        "                 efeat_name: str = 'edge_attr',\n",
        "                 readout_type: str = 'set2set',\n",
        "                 num_step_set2set: int = 6,\n",
        "                 num_layer_set2set: int = 3,\n",
        "                 ffn_hidden_list: List = [300],\n",
        "                 ffn_embeddings: int = 256,\n",
        "                 ffn_activation: str = 'relu',\n",
        "                 ffn_dropout_p: float = 0.0,\n",
        "                 ffn_dropout_at_input_no_act: bool = True):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_tasks: int\n",
        "            Number of tasks.\n",
        "        node_out_feats: int\n",
        "            The length of the final node representation vectors\n",
        "            before readout. Default to 64.\n",
        "        edge_hidden_feats: int\n",
        "            The length of the hidden edge representation vectors\n",
        "            for mpnn edge network. Default to 128.\n",
        "        edge_out_feats: int\n",
        "            The length of the final edge representation vectors\n",
        "            before readout. Default to 64.\n",
        "        num_step_message_passing: int\n",
        "            The number of rounds of message passing. Default to 3.\n",
        "        mpnn_residual: bool\n",
        "            If true, adds residual layer to mpnn layer. Default to True.\n",
        "        message_aggregator_type: str\n",
        "            MPNN message aggregator type, 'sum', 'mean' or 'max'.\n",
        "            Default to 'sum'.\n",
        "        mode: str\n",
        "            The model type, 'classification' or 'regression'.\n",
        "            Default to 'classification'.\n",
        "        number_atom_features: int\n",
        "            The length of the initial atom feature vectors. Default to 134.\n",
        "        number_bond_features: int\n",
        "            The length of the initial bond feature vectors. Default to 6.\n",
        "        n_classes: int\n",
        "            The number of classes to predict per task\n",
        "            (only used when ``mode`` is 'classification'). Default to 1.\n",
        "        nfeat_name: str\n",
        "            For an input graph ``g``, the model assumes that it stores\n",
        "            node features in ``g.ndata[nfeat_name]`` and will retrieve\n",
        "            input node features from that. Default to 'x'.\n",
        "        efeat_name: str\n",
        "            For an input graph ``g``, the model assumes that it stores\n",
        "            edge features in ``g.edata[efeat_name]`` and will retrieve\n",
        "            input edge features from that. Default to 'edge_attr'.\n",
        "        readout_type: str\n",
        "            The Readout type, 'set2set' or 'global_sum_pooling'.\n",
        "            Default to 'set2set'.\n",
        "        num_step_set2set: int\n",
        "            Number of steps in set2set readout.\n",
        "            Used if, readout_type == 'set2set'.\n",
        "            Default to 6.\n",
        "        num_layer_set2set: int\n",
        "            Number of layers in set2set readout.\n",
        "            Used if, readout_type == 'set2set'.\n",
        "            Default to 3.\n",
        "        ffn_hidden_list: List\n",
        "            List of sizes of hidden layer in the feed-forward network layer.\n",
        "            Default to [300].\n",
        "        ffn_embeddings: int\n",
        "            Size of penultimate layer in the feed-forward network layer.\n",
        "            This determines the Principal Odor Map dimension.\n",
        "            Default to 256.\n",
        "        ffn_activation: str\n",
        "            Activation function to be used in feed-forward network layer.\n",
        "            Can choose between 'relu' for ReLU, 'leakyrelu' for LeakyReLU,\n",
        "            'prelu' for PReLU, 'tanh' for TanH, 'selu' for SELU,\n",
        "            and 'elu' for ELU.\n",
        "        ffn_dropout_p: float\n",
        "            Dropout probability for the feed-forward network layer.\n",
        "            Default to 0.0\n",
        "        ffn_dropout_at_input_no_act: bool\n",
        "            If true, dropout is applied on the input tensor.\n",
        "            For single layer, it is not passed to an activation function.\n",
        "        \"\"\"\n",
        "        if mode not in ['classification', 'regression']:\n",
        "            raise ValueError(\n",
        "                \"mode must be either 'classification' or 'regression'\")\n",
        "\n",
        "        super(MPNNPOM, self).__init__()\n",
        "\n",
        "        self.n_tasks: int = n_tasks\n",
        "        self.mode: str = mode\n",
        "        self.n_classes: int = n_classes\n",
        "        self.nfeat_name: str = nfeat_name\n",
        "        self.efeat_name: str = efeat_name\n",
        "        self.readout_type: str = readout_type\n",
        "        self.ffn_embeddings: int = ffn_embeddings\n",
        "        self.ffn_activation: str = ffn_activation\n",
        "        self.ffn_dropout_p: float = ffn_dropout_p\n",
        "\n",
        "        if mode == 'classification':\n",
        "            self.ffn_output: int = n_tasks * n_classes\n",
        "        else:\n",
        "            self.ffn_output = n_tasks\n",
        "\n",
        "        self.mpnn: nn.Module = CustomMPNNGNN(\n",
        "            node_in_feats=number_atom_features,\n",
        "            node_out_feats=node_out_feats,\n",
        "            edge_in_feats=number_bond_features,\n",
        "            edge_hidden_feats=edge_hidden_feats,\n",
        "            num_step_message_passing=num_step_message_passing,\n",
        "            residual=mpnn_residual,\n",
        "            message_aggregator_type=message_aggregator_type)\n",
        "\n",
        "        self.project_edge_feats: nn.Module = nn.Sequential(\n",
        "            nn.Linear(number_bond_features, edge_out_feats), nn.ReLU())\n",
        "\n",
        "        if self.readout_type == 'set2set':\n",
        "            self.readout_set2set: nn.Module = Set2Set(\n",
        "                input_dim=node_out_feats + edge_out_feats,\n",
        "                n_iters=num_step_set2set,\n",
        "                n_layers=num_layer_set2set)\n",
        "            ffn_input: int = 2 * (node_out_feats + edge_out_feats)\n",
        "        elif self.readout_type == 'global_sum_pooling':\n",
        "            ffn_input = node_out_feats + edge_out_feats\n",
        "        else:\n",
        "            raise Exception(\"readout_type invalid\")\n",
        "\n",
        "        if ffn_embeddings is not None:\n",
        "            d_hidden_list: List = ffn_hidden_list + [ffn_embeddings]\n",
        "\n",
        "        self.ffn: nn.Module = CustomPositionwiseFeedForward(\n",
        "            d_input=ffn_input,\n",
        "            d_hidden_list=d_hidden_list,\n",
        "            d_output=self.ffn_output,\n",
        "            activation=ffn_activation,\n",
        "            dropout_p=ffn_dropout_p,\n",
        "            dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
        "\n",
        "    def _readout(self, g: DGLGraph, node_encodings: torch.Tensor,\n",
        "                 edge_feats: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Method to execute the readout phase.\n",
        "        (compute molecules encodings from atom hidden states)\n",
        "\n",
        "        Readout phase consists of radius 0 combination to fold atom\n",
        "        and bond embeddings together,\n",
        "        followed by:\n",
        "            - a reduce-sum across atoms\n",
        "                if `self.readout_type == 'global_sum_pooling'`\n",
        "            - set2set pooling\n",
        "                if `self.readout_type == 'set2set'`\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        g: DGLGraph\n",
        "            A DGLGraph for a batch of graphs.\n",
        "            It stores the node features in\n",
        "            ``dgl_graph.ndata[self.nfeat_name]`` and edge features in\n",
        "            ``dgl_graph.edata[self.efeat_name]``.\n",
        "\n",
        "        node_encodings: torch.Tensor\n",
        "            Tensor containing node hidden states.\n",
        "\n",
        "        edge_feats: torch.Tensor\n",
        "            Tensor containing edge features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        batch_mol_hidden_states: torch.Tensor\n",
        "            Tensor containing batchwise molecule encodings.\n",
        "        \"\"\"\n",
        "\n",
        "        g.ndata['node_emb'] = node_encodings\n",
        "        g.edata['edge_emb'] = self.project_edge_feats(edge_feats)\n",
        "\n",
        "        def message_func(edges) -> Dict:\n",
        "            \"\"\"\n",
        "            The message function to generate messages\n",
        "            along the edges for DGLGraph.send_and_recv()\n",
        "            \"\"\"\n",
        "            src_msg: torch.Tensor = torch.cat(\n",
        "                (edges.src['node_emb'], edges.data['edge_emb']), dim=1)\n",
        "            return {'src_msg': src_msg}\n",
        "\n",
        "        def reduce_func(nodes) -> Dict:\n",
        "            \"\"\"\n",
        "            The reduce function to aggregate the messages\n",
        "            for DGLGraph.send_and_recv()\n",
        "            \"\"\"\n",
        "            src_msg_sum: torch.Tensor = torch.sum(nodes.mailbox['src_msg'],\n",
        "                                                  dim=1)\n",
        "            return {'src_msg_sum': src_msg_sum}\n",
        "\n",
        "        # radius 0 combination to fold atom and bond embeddings together\n",
        "        g.send_and_recv(g.edges(),\n",
        "                        message_func=message_func,\n",
        "                        reduce_func=reduce_func)\n",
        "\n",
        "        if self.readout_type == 'set2set':\n",
        "            batch_mol_hidden_states: torch.Tensor = self.readout_set2set(\n",
        "                g, g.ndata['src_msg_sum'])\n",
        "        elif self.readout_type == 'global_sum_pooling':\n",
        "            batch_mol_hidden_states = dgl.sum_nodes(g, 'src_msg_sum')\n",
        "\n",
        "        # batch_size x (node_out_feats + edge_out_feats)\n",
        "        return batch_mol_hidden_states\n",
        "\n",
        "    def forward(\n",
        "        self, g: DGLGraph\n",
        "    ) -> Union[tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Foward pass for MPNNPOM class. It also returns embeddings for POM.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        g: DGLGraph\n",
        "            A DGLGraph for a batch of graphs. It stores the node features in\n",
        "            ``dgl_graph.ndata[self.nfeat_name]`` and edge features in\n",
        "            ``dgl_graph.edata[self.efeat_name]``.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Union[tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]\n",
        "            The model output.\n",
        "\n",
        "        * When self.mode = 'regression',\n",
        "            its shape will be ``(dgl_graph.batch_size, self.n_tasks)``.\n",
        "        * When self.mode = 'classification',\n",
        "            the output consists of probabilities for classes.\n",
        "            Its shape will be\n",
        "            ``(dgl_graph.batch_size, self.n_tasks, self.n_classes)``\n",
        "            if self.n_tasks > 1;\n",
        "            its shape will be ``(dgl_graph.batch_size, self.n_classes)``\n",
        "            if self.n_tasks is 1.\n",
        "        \"\"\"\n",
        "        node_feats: torch.Tensor = g.ndata[self.nfeat_name]\n",
        "        edge_feats: torch.Tensor = g.edata[self.efeat_name]\n",
        "\n",
        "        node_encodings: torch.Tensor = self.mpnn(g, node_feats, edge_feats)\n",
        "\n",
        "        molecular_encodings: torch.Tensor = self._readout(\n",
        "            g, node_encodings, edge_feats)\n",
        "        if self.readout_type == 'global_sum_pooling':\n",
        "            molecular_encodings = F.softmax(molecular_encodings, dim=1)\n",
        "\n",
        "        embeddings: torch.Tensor\n",
        "        out: torch.Tensor\n",
        "        embeddings, out = self.ffn(molecular_encodings)\n",
        "\n",
        "        if self.mode == 'classification':\n",
        "            if self.n_tasks == 1:\n",
        "                logits: torch.Tensor = out.view(-1, self.n_classes)\n",
        "            else:\n",
        "                logits = out.view(-1, self.n_tasks, self.n_classes)\n",
        "            proba: torch.Tensor = F.sigmoid(\n",
        "                logits)  # (batch, n_tasks, classes)\n",
        "            if self.n_classes == 1:\n",
        "                proba = proba.squeeze(-1)  # (batch, n_tasks)\n",
        "            return proba, logits, embeddings\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class MPNNPOMModel(TorchModel):\n",
        "    \"\"\"\n",
        "    MPNNPOMModel for obtaining a principal odor map\n",
        "    using multilabel-classification based on the pre-print:\n",
        "    \"A Principal Odor Map Unifies DiverseTasks in Human\n",
        "        Olfactory Perception\" [1]\n",
        "\n",
        "    * Combine latest node representations and edge features in\n",
        "        updating node representations, which involves multiple\n",
        "        rounds of message passing.\n",
        "    * For each graph, compute its representation by radius 0 combination\n",
        "        to fold atom and bond embeddings together, followed by\n",
        "        'set2set' or 'global_sum_pooling' readout.\n",
        "    * Perform the final prediction using a feed-forward layer.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Brian K. Lee, Emily J. Mayhew, Benjamin Sanchez-Lengeling,\n",
        "        Jennifer N. Wei, Wesley W. Qian, Kelsie Little, Matthew Andres,\n",
        "        Britney B. Nguyen, Theresa Moloy, Jane K. Parker, Richard C. Gerkin,\n",
        "        Joel D. Mainland, Alexander B. Wiltschko\n",
        "        `A Principal Odor Map Unifies Diverse Tasks\n",
        "        in Human Olfactory Perception preprint\n",
        "        <https://www.biorxiv.org/content/10.1101/2022.09.01.504602v4>`_.\n",
        "\n",
        "    .. [2] Benjamin Sanchez-Lengeling, Jennifer N. Wei, Brian K. Lee,\n",
        "        Richard C. Gerkin, Alán Aspuru-Guzik, Alexander B. Wiltschko\n",
        "        `Machine Learning for Scent:\n",
        "        Learning Generalizable Perceptual Representations\n",
        "        of Small Molecules <https://arxiv.org/abs/1910.10685>`_.\n",
        "\n",
        "    .. [3] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,\n",
        "        Oriol Vinyals, George E. Dahl.\n",
        "        \"Neural Message Passing for Quantum Chemistry.\" ICML 2017.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This class requires DGL (https://github.com/dmlc/dgl) and DGL-LifeSci\n",
        "    (https://github.com/awslabs/dgl-lifesci) to be installed.\n",
        "\n",
        "    The featurizer used with MPNNPOMModel must produce a Deepchem GraphData\n",
        "    object which should have both 'edge' and 'node' features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_tasks: int,\n",
        "                 class_imbalance_ratio: Optional[List] = None,\n",
        "                 loss_aggr_type: str = 'sum',\n",
        "                 learning_rate: Union[float, LearningRateSchedule] = 0.001,\n",
        "                 batch_size: int = 100,\n",
        "                 node_out_feats: int = 64,\n",
        "                 edge_hidden_feats: int = 128,\n",
        "                 edge_out_feats: int = 64,\n",
        "                 num_step_message_passing: int = 3,\n",
        "                 mpnn_residual: bool = True,\n",
        "                 message_aggregator_type: str = 'sum',\n",
        "                 mode: str = 'regression',\n",
        "                 number_atom_features: int = 134,\n",
        "                 number_bond_features: int = 6,\n",
        "                 n_classes: int = 1,\n",
        "                 readout_type: str = 'set2set',\n",
        "                 num_step_set2set: int = 6,\n",
        "                 num_layer_set2set: int = 3,\n",
        "                 ffn_hidden_list: List = [300],\n",
        "                 ffn_embeddings: int = 256,\n",
        "                 ffn_activation: str = 'relu',\n",
        "                 ffn_dropout_p: float = 0.0,\n",
        "                 ffn_dropout_at_input_no_act: bool = True,\n",
        "                 weight_decay: float = 1e-5,\n",
        "                 self_loop: bool = False,\n",
        "                 optimizer_name: str = 'adam',\n",
        "                 device_name: Optional[str] = 'cpu',\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_tasks: int\n",
        "            Number of tasks.\n",
        "        class_imbalance_ratio: Optional[List]\n",
        "            List of imbalance ratios per task.\n",
        "        loss_aggr_type: str\n",
        "            loss aggregation type; 'sum' or 'mean'. Default to 'sum'.\n",
        "            Only applies to CustomMultiLabelLoss for classification\n",
        "        learning_rate: Union[float, LearningRateSchedule]\n",
        "            Learning rate value or scheduler object. Default to 0.001.\n",
        "        batch_size: int\n",
        "            Batch size for training. Default to 100.\n",
        "        node_out_feats: int\n",
        "            The length of the final node representation vectors\n",
        "            before readout. Default to 64.\n",
        "        edge_hidden_feats: int\n",
        "            The length of the hidden edge representation vectors\n",
        "            for mpnn edge network. Default to 128.\n",
        "        edge_out_feats: int\n",
        "            The length of the final edge representation vectors\n",
        "            before readout. Default to 64.\n",
        "        num_step_message_passing: int\n",
        "            The number of rounds of message passing. Default to 3.\n",
        "        mpnn_residual: bool\n",
        "            If true, adds residual layer to mpnn layer. Default to True.\n",
        "        message_aggregator_type: str\n",
        "            MPNN message aggregator type, 'sum', 'mean' or 'max'.\n",
        "            Default to 'sum'.\n",
        "        mode: str\n",
        "            The model type, 'classification' or 'regression'.\n",
        "            Default to 'classification'.\n",
        "        number_atom_features: int\n",
        "            The length of the initial atom feature vectors. Default to 134.\n",
        "        number_bond_features: int\n",
        "            The length of the initial bond feature vectors. Default to 6.\n",
        "        n_classes: int\n",
        "            The number of classes to predict per task\n",
        "            (only used when ``mode`` is 'classification'). Default to 1.\n",
        "        readout_type: str\n",
        "            The Readout type, 'set2set' or 'global_sum_pooling'.\n",
        "            Default to 'set2set'.\n",
        "        num_step_set2set: int\n",
        "            Number of steps in set2set readout.\n",
        "            Used if, readout_type == 'set2set'.\n",
        "            Default to 6.\n",
        "        num_layer_set2set: int\n",
        "            Number of layers in set2set readout.\n",
        "            Used if, readout_type == 'set2set'.\n",
        "            Default to 3.\n",
        "        ffn_hidden_list: List\n",
        "            List of sizes of hidden layer in the feed-forward network layer.\n",
        "            Default to [300].\n",
        "        ffn_embeddings: int\n",
        "            Size of penultimate layer in the feed-forward network layer.\n",
        "            This determines the Principal Odor Map dimension.\n",
        "            Default to 256.\n",
        "        ffn_activation: str\n",
        "            Activation function to be used in feed-forward network layer.\n",
        "            Can choose between 'relu' for ReLU, 'leakyrelu' for LeakyReLU,\n",
        "            'prelu' for PReLU, 'tanh' for TanH, 'selu' for SELU,\n",
        "            and 'elu' for ELU.\n",
        "        ffn_dropout_p: float\n",
        "            Dropout probability for the feed-forward network layer.\n",
        "            Default to 0.0\n",
        "        ffn_dropout_at_input_no_act: bool\n",
        "            If true, dropout is applied on the input tensor.\n",
        "            For single layer, it is not passed to an activation function.\n",
        "        weight_decay: float\n",
        "            weight decay value for L1 and L2 regularization. Default to 1e-5.\n",
        "        self_loop: bool\n",
        "            Whether to add self loops for the nodes, i.e. edges\n",
        "            from nodes to themselves. Generally, an MPNNPOMModel\n",
        "            does not require self loops. Default to False.\n",
        "        optimizer_name: str\n",
        "            Name of optimizer to be used from\n",
        "            [adam, adagrad, adamw, sparseadam, rmsprop, sgd, kfac]\n",
        "            Default to 'adam'.\n",
        "        device_name: Optional[str]\n",
        "            The device on which to run computations. If None, a device is\n",
        "            chosen automatically.\n",
        "        kwargs\n",
        "            This can include any keyword argument of TorchModel.\n",
        "        \"\"\"\n",
        "        model: nn.Module = MPNNPOM(\n",
        "            n_tasks=n_tasks,\n",
        "            node_out_feats=node_out_feats,\n",
        "            edge_hidden_feats=edge_hidden_feats,\n",
        "            edge_out_feats=edge_out_feats,\n",
        "            num_step_message_passing=num_step_message_passing,\n",
        "            mpnn_residual=mpnn_residual,\n",
        "            message_aggregator_type=message_aggregator_type,\n",
        "            mode=mode,\n",
        "            number_atom_features=number_atom_features,\n",
        "            number_bond_features=number_bond_features,\n",
        "            n_classes=n_classes,\n",
        "            readout_type=readout_type,\n",
        "            num_step_set2set=num_step_set2set,\n",
        "            num_layer_set2set=num_layer_set2set,\n",
        "            ffn_hidden_list=ffn_hidden_list,\n",
        "            ffn_embeddings=ffn_embeddings,\n",
        "            ffn_activation=ffn_activation,\n",
        "            ffn_dropout_p=ffn_dropout_p,\n",
        "            ffn_dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
        "\n",
        "        if class_imbalance_ratio and (len(class_imbalance_ratio) != n_tasks):\n",
        "            raise Exception(\"size of class_imbalance_ratio \\\n",
        "                            should be equal to n_tasks\")\n",
        "\n",
        "        if mode == 'regression':\n",
        "            loss: Loss = L2Loss()\n",
        "            output_types: List = ['prediction']\n",
        "        else:\n",
        "            loss = CustomMultiLabelLoss(\n",
        "                class_imbalance_ratio=class_imbalance_ratio,\n",
        "                loss_aggr_type=loss_aggr_type,\n",
        "                device=device_name)\n",
        "            output_types = ['prediction', 'loss', 'embedding']\n",
        "\n",
        "        optimizer: Optimizer = get_optimizer(optimizer_name)\n",
        "        optimizer.learning_rate = learning_rate\n",
        "        if device_name is not None:\n",
        "            device: Optional[torch.device] = torch.device(device_name)\n",
        "        else:\n",
        "            device = None\n",
        "        super(MPNNPOMModel, self).__init__(model,\n",
        "                                           loss=loss,\n",
        "                                           output_types=output_types,\n",
        "                                           optimizer=optimizer,\n",
        "                                           learning_rate=learning_rate,\n",
        "                                           batch_size=batch_size,\n",
        "                                           device=device,\n",
        "                                           **kwargs)\n",
        "\n",
        "        self.weight_decay: float = weight_decay\n",
        "        self._self_loop: bool = self_loop\n",
        "        self.regularization_loss: Callable = self._regularization_loss\n",
        "\n",
        "    def _regularization_loss(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        L1 and L2-norm losses for regularization\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            sum of l1_norm and l2_norm\n",
        "        \"\"\"\n",
        "        l1_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
        "        l2_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'bias' not in name:\n",
        "                l1_regularization = l1_regularization + torch.norm(param, p=1)\n",
        "                l2_regularization = l2_regularization + torch.norm(param, p=2)\n",
        "        l1_norm: torch.Tensor = self.weight_decay * l1_regularization\n",
        "        l2_norm: torch.Tensor = self.weight_decay * l2_regularization\n",
        "        return l1_norm + l2_norm\n",
        "\n",
        "    def _prepare_batch(\n",
        "        self, batch: Tuple[List, List, List]\n",
        "    ) -> Tuple[DGLGraph, List[torch.Tensor], List[torch.Tensor]]:\n",
        "        \"\"\"Create batch data for MPNN.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: Tuple[List, List, List]\n",
        "            The tuple is ``(inputs, labels, weights)``.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        g: DGLGraph\n",
        "            DGLGraph for a batch of graphs.\n",
        "        labels: list of torch.Tensor or None\n",
        "            The graph labels.\n",
        "        weights: list of torch.Tensor or None\n",
        "            The weights for each sample or\n",
        "            sample/task pair converted to torch.Tensor.\n",
        "        \"\"\"\n",
        "        inputs: List\n",
        "        labels: List\n",
        "        weights: List\n",
        "\n",
        "        inputs, labels, weights = batch\n",
        "        dgl_graphs: List[DGLGraph] = [\n",
        "            graph.to_dgl_graph(self_loop=self._self_loop)\n",
        "            for graph in inputs[0]\n",
        "        ]\n",
        "        g: DGLGraph = dgl.batch(dgl_graphs).to(self.device)\n",
        "        _, labels, weights = super(MPNNPOMModel, self)._prepare_batch(\n",
        "            ([], labels, weights))\n",
        "        return g, labels, weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ]
        }
      ],
      "source": [
        "import dgl\n",
        "print(dgl.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B4DtOK5lAC0B"
      },
      "outputs": [],
      "source": [
        "import deepchem as dc\n",
        "from openpom.feat.graph_featurizer import GraphFeaturizer\n",
        "\n",
        "# Define the SMILES string to predict\n",
        "single_smiles = 'COC1=C(C=CC(=C1)C=O)O'  # Replace 'CCO' with the actual SMILES string\n",
        "\n",
        "# Featurize the SMILES string\n",
        "featurizer = GraphFeaturizer()\n",
        "single_molecule = featurizer.featurize([single_smiles])\n",
        "\n",
        "# Create a NumpyDataset for the single molecule\n",
        "single_dataset = dc.data.NumpyDataset(single_molecule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xs9zCYm0T3eu"
      },
      "outputs": [],
      "source": [
        "train_ratios = [0.05584756898817345,\n",
        " 0.07424441524310119,\n",
        " 0.0657030223390276,\n",
        " 0.04402102496714849,\n",
        " 0.052562417871222074,\n",
        " 0.06438896189224705,\n",
        " 0.04533508541392904,\n",
        " 0.16031537450722733,\n",
        " 0.04007884362680683,\n",
        " 0.03942181340341656,\n",
        " 0.1557161629434954,\n",
        " 0.0657030223390276,\n",
        " 0.022339027595269383,\n",
        " 0.0164257555847569,\n",
        " 0.08147174770039421,\n",
        " 0.03088042049934297,\n",
        " 0.022996057818659658,\n",
        " 0.021681997371879105,\n",
        " 0.09526938239159001,\n",
        " 0.07095926412614981,\n",
        " 0.023653088042049936,\n",
        " 0.06898817345597898,\n",
        " 0.12352168199737187,\n",
        " 0.025624178712220762,\n",
        " 0.020367936925098553,\n",
        " 0.015111695137976347,\n",
        " 0.0900131406044678,\n",
        " 0.042706964520367936,\n",
        " 0.03613666228646518,\n",
        " 0.03416557161629435,\n",
        " 0.19842312746386334,\n",
        " 0.04993429697766097,\n",
        " 0.017739816031537452,\n",
        " 0.06110381077529566,\n",
        " 0.04730617608409987,\n",
        " 0.05781865965834428,\n",
        " 0.05847568988173456,\n",
        " 0.04467805519053877,\n",
        " 0.04204993429697766,\n",
        " 0.028252299605781867,\n",
        " 0.03153745072273324,\n",
        " 0.09921156373193167,\n",
        " 0.023653088042049936,\n",
        " 0.06241787122207622,\n",
        " 0.08541392904073587,\n",
        " 0.17082785808147175,\n",
        " 0.1504599211563732,\n",
        " 0.3140604467805519,\n",
        " 0.07621550591327202,\n",
        " 0.040735873850197106,\n",
        " 0.5709592641261498,\n",
        " 0.27989487516425754,\n",
        " 0.02759526938239159,\n",
        " 1.0,\n",
        " 0.06241787122207622,\n",
        " 0.017739816031537452,\n",
        " 0.032194480946123524,\n",
        " 0.04730617608409987,\n",
        " 0.03942181340341656,\n",
        " 0.0328515111695138,\n",
        " 0.759526938239159,\n",
        " 0.017082785808147174,\n",
        " 0.03679369250985545,\n",
        " 0.02759526938239159,\n",
        " 0.3968462549277267,\n",
        " 0.0821287779237845,\n",
        " 0.02956636005256242,\n",
        " 0.05387647831800263,\n",
        " 0.025624178712220762,\n",
        " 0.024967148488830485,\n",
        " 0.03482260183968462,\n",
        " 0.022339027595269383,\n",
        " 0.07030223390275953,\n",
        " 0.028252299605781867,\n",
        " 0.04204993429697766,\n",
        " 0.03613666228646518,\n",
        " 0.017082785808147174,\n",
        " 0.15637319316688567,\n",
        " 0.03613666228646518,\n",
        " 0.06964520367936924,\n",
        " 0.07030223390275953,\n",
        " 0.04533508541392904,\n",
        " 0.11695137976346912,\n",
        " 0.02890932982917214,\n",
        " 0.049277266754270695,\n",
        " 0.051248357424441525,\n",
        " 0.1038107752956636,\n",
        " 0.03942181340341656,\n",
        " 0.19185282522996058,\n",
        " 0.10512483574244415,\n",
        " 0.20367936925098554,\n",
        " 0.09264126149802891,\n",
        " 0.05059132720105125,\n",
        " 0.017739816031537452,\n",
        " 0.03088042049934297,\n",
        " 0.018396846254927726,\n",
        " 0.049277266754270695,\n",
        " 0.0683311432325887,\n",
        " 0.09461235216819974,\n",
        " 0.04007884362680683,\n",
        " 0.10775295663600526,\n",
        " 0.03153745072273324,\n",
        " 0.018396846254927726,\n",
        " 0.02890932982917214,\n",
        " 0.08475689881734559,\n",
        " 0.10183968462549277,\n",
        " 0.028252299605781867,\n",
        " 0.022339027595269383,\n",
        " 0.04533508541392904,\n",
        " 0.1590013140604468,\n",
        " 0.1727989487516426,\n",
        " 0.04336399474375821,\n",
        " 0.018396846254927726,\n",
        " 0.0657030223390276,\n",
        " 0.054533508541392904,\n",
        " 0.03613666228646518,\n",
        " 0.026281208935611037,\n",
        " 0.030223390275952694,\n",
        " 0.03810775295663601,\n",
        " 0.24310118265440211,\n",
        " 0.025624178712220762,\n",
        " 0.21156373193166886,\n",
        " 0.01971090670170828,\n",
        " 0.7325886990801577,\n",
        " 0.02890932982917214,\n",
        " 0.025624178712220762,\n",
        " 0.0519053876478318,\n",
        " 0.018396846254927726,\n",
        " 0.20170827858081472,\n",
        " 0.05847568988173456,\n",
        " 0.14651773981603153,\n",
        " 0.019053876478318004,\n",
        " 0.03350854139290407,\n",
        " 0.04993429697766097,\n",
        " 0.2555847568988173,\n",
        " 0.023653088042049936,\n",
        " 0.11629434954007885,\n",
        " 0.3567674113009198]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xms3gKTCAeJ-"
      },
      "outputs": [],
      "source": [
        "# initialize model\n",
        "\n",
        "model = MPNNPOMModel(n_tasks = 138,\n",
        "                            batch_size=128,\n",
        "                            learning_rate=1e-4,\n",
        "                            class_imbalance_ratio = train_ratios,\n",
        "                            loss_aggr_type = 'sum',\n",
        "                            node_out_feats = 100,\n",
        "                            edge_hidden_feats = 75,\n",
        "                            edge_out_feats = 100,\n",
        "                            num_step_message_passing = 5,\n",
        "                            mpnn_residual = True,\n",
        "                            message_aggregator_type = 'sum',\n",
        "                            mode = 'classification',\n",
        "                            number_atom_features = GraphConvConstants.ATOM_FDIM,\n",
        "                            number_bond_features = GraphConvConstants.BOND_FDIM,\n",
        "                            n_classes = 1,\n",
        "                            readout_type = 'set2set',\n",
        "                            num_step_set2set = 3,\n",
        "                            num_layer_set2set = 2,\n",
        "                            ffn_hidden_list= [392, 392],\n",
        "                            ffn_embeddings = 256,\n",
        "                            ffn_activation = 'relu',\n",
        "                            ffn_dropout_p = 0.12,\n",
        "                            ffn_dropout_at_input_no_act = False,\n",
        "                            weight_decay = 1e-5,\n",
        "                            self_loop = False,\n",
        "                            optimizer_name = 'adam',\n",
        "                            log_frequency = 32,\n",
        "                            model_dir = 'data',\n",
        "                            device_name='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0RUBd2ZAmqs",
        "outputId": "613a7ffa-51e6-48bd-f6b9-a1782a5a41fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Dokumenter\\Skole\\Prosjektoppgave\\prosjekt\\venv\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:1078: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(checkpoint, map_location=self.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Probabilities for the 138 notes: [[0.04575704 0.04282943 0.05341665 0.28640267 0.0329218  0.20288868\n",
            "  0.36172265 0.02111001 0.04237433 0.317042   0.4279455  0.03573177\n",
            "  0.03719727 0.02371974 0.13851589 0.32038516 0.0289441  0.01613994\n",
            "  0.32080555 0.09608915 0.03982432 0.08091616 0.27430353 0.02159856\n",
            "  0.07350693 0.03478024 0.05178089 0.25843382 0.11839368 0.21073028\n",
            "  0.06080894 0.06195458 0.34229103 0.06232229 0.146159   0.16066001\n",
            "  0.01784373 0.0504692  0.09188339 0.07404196 0.28687137 0.35412017\n",
            "  0.00835921 0.10641305 0.16819969 0.1959213  0.18295585 0.06957997\n",
            "  0.05369798 0.06176599 0.44382554 0.099797   0.0437536  0.34240264\n",
            "  0.04317852 0.04487146 0.06809818 0.1567298  0.02871715 0.03830222\n",
            "  0.12342923 0.30967513 0.28897983 0.04847127 0.24057142 0.11724697\n",
            "  0.0538983  0.05945813 0.02251486 0.02622347 0.09840045 0.01832962\n",
            "  0.05089311 0.19233459 0.02440911 0.03873232 0.08897623 0.10484001\n",
            "  0.42817798 0.06610651 0.13504575 0.09801025 0.06511155 0.01436159\n",
            "  0.05929188 0.07033478 0.3142712  0.02289041 0.35918513 0.11876268\n",
            "  0.0813908  0.02574145 0.1128162  0.13099824 0.06884202 0.07183688\n",
            "  0.05053758 0.0220471  0.74288285 0.02075444 0.05096442 0.05944619\n",
            "  0.04386013 0.02740695 0.42564523 0.22570969 0.05380434 0.04117484\n",
            "  0.0450562  0.13925046 0.09873389 0.07651279 0.02396113 0.12061375\n",
            "  0.19190037 0.47502732 0.03915709 0.0662094  0.09469683 0.58431464\n",
            "  0.13115095 0.06589254 0.03034441 0.740703   0.10499656 0.02870986\n",
            "  0.1934925  0.01764768 0.09963637 0.6608363  0.04891734 0.06998596\n",
            "  0.0240582  0.23647371 0.06107602 0.02455437 0.11874472 0.43846476]]\n"
          ]
        }
      ],
      "source": [
        "# Restore the model from the checkpoint\n",
        "model.restore()\n",
        "\n",
        "# Predict the probabilities for the single molecule\n",
        "predicted_probabilities = model.predict(single_dataset)\n",
        "\n",
        "# Output the predicted probabilities\n",
        "print(\"Predicted Probabilities for the 138 notes:\", predicted_probabilities)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
